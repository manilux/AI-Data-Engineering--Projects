
# Stock Market ETL Pipeline

### Objective
Build an end-to-end *ETL pipeline* to process stock market data using *PySpark, Databricks, Delta Lake, and Snowflake* for scalable analytics.

---

### Tech Stack
- *PySpark* for data transformations  
- *Databricks (Delta Lake)* for scalable data processing  
- *Snowflake* for cloud data warehousing  
- *Kafka (Planned)* for real-time streaming  

---

### Data Flow
1. *Extract*: Ingest raw stock data (CSV/JSON)  
2. *Transform*: Clean, enrich, and optimize with PySpark & Delta Lake  
3. *Load*: Push processed data into Snowflake for analytics  

---

### Architecture Diagram
(Diagram will be added here soon)

---

### Next Steps
- Upload sample dataset
- Write PySpark ETL scripts
- Integrate with Snowflake
- Add streaming pipeline (Phase 2)
